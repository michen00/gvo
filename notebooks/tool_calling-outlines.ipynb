{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcd94af",
   "metadata": {},
   "source": [
    "# Tool Calling with Guidance and Outlines\n",
    "\n",
    "This notebook mirrors the original `docs/tutorials/guidance/tool_calling.ipynb` tutorial and extends it with an Outlines-based implementation so we can evaluate both stacks side-by-side. The sections follow the agreed outline and are intended to be executed top-to-bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc748c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "We install or verify the required dependencies and configure shared imports. Guidance and Outlines both rely on a working LLM backend (OpenAI, vLLM, llama.cpp, etc.). The code assumes an OpenAI-compatible HTTP API is available via environment variables such as `OPENAI_API_KEY`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a883be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies into the current environment.\n",
    "# Uncomment the lines below if you need to install packages.\n",
    "# %pip install guidance outlines llama-cpp-python httpx sympy pydantic pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79a4a9",
   "metadata": {},
   "source": [
    "## 2. Replicate Guidance Tool-Calling Workflow\n",
    "\n",
    "We reuse the original tutorial's tools to confirm the baseline behaviour before porting anything to Outlines. The cells below define the helpers, register them with Guidance, and (optionally) execute a request against a live OpenAI-compatible endpoint. Set `RUN_GUIDANCE_DEMO = True` if you want to make the remote call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "from ast import literal_eval\n",
    "from functools import lru_cache\n",
    "from typing import Literal\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from pydantic import BaseModel, ConfigDict, Field\n",
    "\n",
    "try:\n",
    "    from openai import BadRequestError\n",
    "except ImportError:  # pragma: no cover - optional dependency\n",
    "    BadRequestError = Exception  # type: ignore[assignment]\n",
    "\n",
    "try:\n",
    "    from sympy import sympify\n",
    "except ImportError:  # pragma: no cover - optional dependency\n",
    "    sympify = None  # type: ignore[assignment]\n",
    "\n",
    "RNG = random.SystemRandom()\n",
    "DOMAIN_PATTERN = re.compile(r\"(?:(?:[A-Za-z0-9-]+\\.)+[A-Za-z]{2,})\")\n",
    "WHOIS_EXECUTABLE = shutil.which(\"whois\")\n",
    "\n",
    "\n",
    "def get_weather_func(\n",
    "    city: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"celsius\"\n",
    ") -> str:\n",
    "    \"\"\"Return a mock weather string so the demos stay deterministic-friendly.\"\"\"\n",
    "    temp = RNG.randint(-10, 35)\n",
    "    weather = \"snowy\" if temp <= 0 else \"cloudy\" if temp <= 20 else \"sunny\"\n",
    "    if unit == \"fahrenheit\":\n",
    "        temp = temp * 9 / 5 + 32\n",
    "    return (\n",
    "        f\"The current temperature in {city} is {temp} degrees \"\n",
    "        f\"{unit} and it is {weather}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def whois_lookup(url: str) -> str:\n",
    "    \"\"\"Perform a WHOIS lookup for the provided URL and return the raw output.\"\"\"\n",
    "    if WHOIS_EXECUTABLE is None:\n",
    "        msg = \"The 'whois' executable is not available on this system.\"\n",
    "        raise RuntimeError(msg)\n",
    "    domain = urlparse(url).netloc or url\n",
    "    if not domain or not DOMAIN_PATTERN.fullmatch(domain):\n",
    "        msg = f\"Invalid domain: {domain}\"\n",
    "        raise ValueError(msg)\n",
    "    result = subprocess.run(  # noqa: S603\n",
    "        [WHOIS_EXECUTABLE, domain],\n",
    "        check=False,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=30,\n",
    "    )\n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "def evaluate_expression(expr: str) -> str:\n",
    "    \"\"\"Safely evaluate a simple arithmetic expression using sympy as a guard.\"\"\"\n",
    "    if sympify is None:\n",
    "        msg = \"Install sympy to enable the calculator tool.\"\n",
    "        raise RuntimeError(msg)\n",
    "    sympy_expr = sympify(expr)\n",
    "    if not sympy_expr.is_number:\n",
    "        msg = f\"Invalid expression: {expr}\"\n",
    "        raise ValueError(msg)\n",
    "    return str(literal_eval(str(sympy_expr)))\n",
    "\n",
    "\n",
    "class GetWeatherArgs(BaseModel):\n",
    "    \"\"\"Structured input expected by the weather tool.\"\"\"\n",
    "\n",
    "    city: str = Field(description=\"The city to inspect.\")\n",
    "    unit: Literal[\"celsius\", \"fahrenheit\"] = Field(description=\"Unit selection.\")\n",
    "\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "\n",
    "RUN_GUIDANCE_DEMO = False\n",
    "GUIDANCE_MODEL_NAME = os.getenv(\"GUIDANCE_MODEL_NAME\", \"gpt-5-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f17e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import Tool, assistant, gen, user\n",
    "from guidance.models import OpenAI\n",
    "\n",
    "get_weather_tool = Tool.from_callable(\n",
    "    callable=get_weather_func,\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a given city.\",\n",
    "    parameters=GetWeatherArgs,\n",
    ")\n",
    "whois_tool = Tool.from_regex(\n",
    "    pattern=r\"https?:\\/\\/[^\\s]+\",\n",
    "    callable=whois_lookup,\n",
    "    name=\"whois_lookup\",\n",
    "    description=\"Perform a WHOIS lookup on a URL.\",\n",
    ")\n",
    "calculator_tool = Tool.from_callable(\n",
    "    callable=evaluate_expression,\n",
    "    name=\"calculator\",\n",
    "    description=\"Evaluate a basic arithmetic expression.\",\n",
    ")\n",
    "guidance_tools = [get_weather_tool, whois_tool, calculator_tool]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_guidance_model() -> OpenAI:\n",
    "    \"\"\"Instantiate and cache the Guidance model backend.\"\"\"\n",
    "    return OpenAI(GUIDANCE_MODEL_NAME, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92095795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenAI rejected the streaming request (often due to unverified org). Set RUN_GUIDANCE_DEMO = False or switch to a streaming-capable backend.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_guidance_demo(prompt: str) -> str:\n",
    "    \"\"\"Execute a guidance tool call while handling optional API failures gracefully.\"\"\"\n",
    "    if not RUN_GUIDANCE_DEMO:\n",
    "        return \"Skipping live Guidance call. Set RUN_GUIDANCE_DEMO = True to execute.\"\n",
    "    lm = get_guidance_model()\n",
    "    try:\n",
    "        with user():\n",
    "            lm += prompt\n",
    "        with assistant():\n",
    "            lm += gen(tools=guidance_tools, tool_choice=\"required\")\n",
    "        with assistant():\n",
    "            lm += gen()\n",
    "        return str(lm)\n",
    "    except BadRequestError:\n",
    "        return (\n",
    "            \"OpenAI rejected the streaming request (often due to unverified org). \"\n",
    "            \"Set RUN_GUIDANCE_DEMO = False or switch to a streaming-capable backend.\"\n",
    "        )\n",
    "\n",
    "\n",
    "guidance_demo_prompt = \"Who owns the openai.com website?\"\n",
    "run_guidance_demo(guidance_demo_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39836337",
   "metadata": {},
   "source": [
    "## 3. Port the Workflow to Outlines\n",
    "\n",
    "Outlines does not execute callbacks automatically, so we compile a structured schema that asks the model which tool to call and with what arguments. The runtime then invokes the Python callable, records the observation, and loops until a final answer is produced. This mirrors the behaviour of Guidance while keeping the decoding constrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections.abc import Callable\n",
    "from enum import Enum\n",
    "from typing import Annotated\n",
    "\n",
    "from outlines import Generator, json_schema\n",
    "from outlines.models.openai import from_openai as outlines_from_openai\n",
    "from pydantic import TypeAdapter\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI as OpenAIClient\n",
    "except ImportError:\n",
    "    OpenAIClient = None  # type: ignore[assignment]\n",
    "\n",
    "RUN_OUTLINES_DEMO = False\n",
    "OUTLINES_MODEL_NAME = os.getenv(\"OUTLINES_MODEL_NAME\", \"gpt-5-mini\")\n",
    "\n",
    "\n",
    "class ToolName(str, Enum):\n",
    "    \"\"\"Enumerate the available tool names exposed to the model.\"\"\"\n",
    "\n",
    "    get_weather = \"get_weather\"\n",
    "    whois_lookup = \"whois_lookup\"\n",
    "    calculator = \"calculator\"\n",
    "    final_answer = \"final_answer\"\n",
    "\n",
    "\n",
    "class WeatherCall(BaseModel):\n",
    "    \"\"\"Payload for the weather lookup tool.\"\"\"\n",
    "\n",
    "    tool: Literal[\"get_weather\"] = \"get_weather\"\n",
    "    arguments: GetWeatherArgs\n",
    "\n",
    "\n",
    "class WhoisArgs(BaseModel):\n",
    "    \"\"\"Arguments expected by the WHOIS lookup tool.\"\"\"\n",
    "\n",
    "    url: str = Field(pattern=r\"https?://[^\\s]+\", description=\"Absolute URL to inspect.\")\n",
    "\n",
    "\n",
    "class WhoisCall(BaseModel):\n",
    "    \"\"\"Payload for the WHOIS lookup tool.\"\"\"\n",
    "\n",
    "    tool: Literal[\"whois_lookup\"] = \"whois_lookup\"\n",
    "    arguments: WhoisArgs\n",
    "\n",
    "\n",
    "class CalculatorArgs(BaseModel):\n",
    "    \"\"\"Arguments expected by the calculator tool.\"\"\"\n",
    "\n",
    "    expression: str = Field(description=\"Arithmetic expression such as 2+2\")\n",
    "\n",
    "\n",
    "class CalculatorCall(BaseModel):\n",
    "    \"\"\"Payload for the calculator tool.\"\"\"\n",
    "\n",
    "    tool: Literal[\"calculator\"] = \"calculator\"\n",
    "    arguments: CalculatorArgs\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"Terminal payload emitted when no more tools are required.\"\"\"\n",
    "\n",
    "    tool: Literal[\"final_answer\"] = \"final_answer\"\n",
    "    answer: str = Field(description=\"The final natural language response.\")\n",
    "\n",
    "\n",
    "ToolDecisionType = WeatherCall | WhoisCall | CalculatorCall | FinalAnswer\n",
    "\n",
    "outlines_model = None\n",
    "TOOL_DECISION_SCHEMA = json_schema(TypeAdapter(ToolDecisionType).json_schema())\n",
    "\n",
    "TOOL_REGISTRY: dict[str, Callable[[dict[str, object]], str]] = {\n",
    "    \"get_weather\": lambda payload: get_weather_func(**payload),\n",
    "    \"whois_lookup\": lambda payload: whois_lookup(payload[\"url\"]),\n",
    "    \"calculator\": lambda payload: str(evaluate_expression(payload[\"expression\"])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821eb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Failed to initialize Outlines model: 'module' object is not callable\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTLINES_SYSTEM_PROMPT = (\n",
    "    \"You are an assistant that can call tools to answer the user's question.\\n\"\n",
    "    \"Use the provided tool names exactly. When you have enough information, \"\n",
    "    \"return `final_answer`.\"\n",
    ")\n",
    "\n",
    "\n",
    "def render_transcript(question: str, scratchpad: str) -> str:\n",
    "    \"\"\"Format the conversation transcript expected by the Outlines runner.\"\"\"\n",
    "    base = f\"{OUTLINES_SYSTEM_PROMPT}\\n\\nQuestion: {question}\\nScratchpad:\"\n",
    "    if scratchpad:\n",
    "        return f\"{base}\\n{scratchpad.strip()}\"\n",
    "    return base\n",
    "\n",
    "\n",
    "def _coerce_payload(payload: object) -> object:\n",
    "    \"\"\"Convert pydantic models or other objects into plain dictionaries.\"\"\"\n",
    "    if hasattr(payload, \"model_dump\"):\n",
    "        return payload.model_dump()\n",
    "    return payload\n",
    "\n",
    "\n",
    "def _normalize_decision(decision: object) -> dict[str, object]:\n",
    "    \"\"\"Normalise the model response into a dictionary for downstream handling.\"\"\"\n",
    "    if isinstance(decision, str):\n",
    "        return json.loads(decision)\n",
    "    if isinstance(decision, dict):\n",
    "        return {key: _coerce_payload(value) for key, value in decision.items()}\n",
    "    coerced = _coerce_payload(decision)\n",
    "    if isinstance(coerced, dict):\n",
    "        return coerced\n",
    "    msg = f\"Unexpected decision payload type: {type(decision)!r}\"\n",
    "    raise TypeError(msg)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _ensure_outlines_model() -> object:\n",
    "    \"\"\"Instantiate and cache the Outlines model backed by the OpenAI client.\"\"\"\n",
    "    if OpenAIClient is None:\n",
    "        msg = \"Install the openai package >= 1.0 to run this demo.\"\n",
    "        raise RuntimeError(msg)\n",
    "    client_kwargs: dict[str, object] = {}\n",
    "    base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "    if base_url:\n",
    "        client_kwargs[\"base_url\"] = base_url\n",
    "    organization = os.getenv(\"OPENAI_ORG_ID\")\n",
    "    if organization:\n",
    "        client_kwargs[\"organization\"] = organization\n",
    "    client = OpenAIClient(**client_kwargs)\n",
    "    return outlines_from_openai(client, OUTLINES_MODEL_NAME)\n",
    "\n",
    "\n",
    "def run_outlines_agent(question: str, max_turns: int = 5) -> str:\n",
    "    \"\"\"Loop until the model emits `final_answer` or `max_turns` turns elapse.\"\"\"\n",
    "    if not RUN_OUTLINES_DEMO:\n",
    "        return \"Skipping live Outlines call. Set RUN_OUTLINES_DEMO = True to execute.\"\n",
    "    try:\n",
    "        model = _ensure_outlines_model()\n",
    "    except RuntimeError as exc:\n",
    "        return f\"Failed to initialize Outlines model: {exc}\"\n",
    "    generator = Generator(model, TOOL_DECISION_SCHEMA)\n",
    "    scratchpad = \"\"\n",
    "    for turn in range(max_turns):\n",
    "        prompt = render_transcript(question, scratchpad)\n",
    "        raw_decision = generator(prompt, temperature=0, max_tokens=512)\n",
    "        decision = _normalize_decision(raw_decision)\n",
    "        tool_name = decision.get(\"tool\")\n",
    "        if tool_name == ToolName.final_answer.value:\n",
    "            return str(decision.get(\"answer\", \"\"))\n",
    "        arguments = _coerce_payload(decision.get(\"arguments\", {})) or {}\n",
    "        executor = TOOL_REGISTRY.get(tool_name)\n",
    "        if executor is None:\n",
    "            observation = f\"Unknown tool: {tool_name}\"\n",
    "        else:\n",
    "            try:\n",
    "                observation = executor(arguments)\n",
    "            except (RuntimeError, ValueError) as exc:\n",
    "                observation = f\"Tool error: {exc}\"\n",
    "        scratchpad += (\n",
    "            f\"\\nTurn {turn + 1}\"\n",
    "            f\"\\nTool: {tool_name}\"\n",
    "            f\"\\nArgs: {json.dumps(arguments)}\"\n",
    "            f\"\\nObservation: {observation}\"\n",
    "        )\n",
    "    return \"Reached max turns without final answer.\"\n",
    "\n",
    "\n",
    "outlines_demo_prompt = (\n",
    "    \"If Johnny has 5 apples and gives 2 to Mary, how many apples remain?\"\n",
    ")\n",
    "run_outlines_agent(outlines_demo_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc2001",
   "metadata": {},
   "source": [
    "## 4. Side-by-Side Harness\n",
    "\n",
    "This helper runs the same question through both stacks (respecting the demo flags) so you can diff the traces and assess parity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad3a501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guidance': 'OpenAI rejected the streaming request (often due to unverified org). Set RUN_GUIDANCE_DEMO = False or switch to a streaming-capable backend.',\n",
       " 'outlines': \"Failed to initialize Outlines model: 'module' object is not callable\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_workflows(question: str) -> dict[str, str]:\n",
    "    \"\"\"Execute the same query via Guidance and Outlines runners.\"\"\"\n",
    "    return {\n",
    "        \"guidance\": run_guidance_demo(question),\n",
    "        \"outlines\": run_outlines_agent(question),\n",
    "    }\n",
    "\n",
    "\n",
    "comparison_prompt = \"What is the weather like in San Francisco today?\"\n",
    "compare_workflows(comparison_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c15cd",
   "metadata": {},
   "source": [
    "## 5. Where to Go Next\n",
    "\n",
    "- Toggle `RUN_GUIDANCE_DEMO` and `RUN_OUTLINES_DEMO` to compare real traces once credentials are set up.\n",
    "- Swap `GUIDANCE_MODEL_NAME` or `OUTLINES_MODEL_NAME` to experiment with self-hosted backends such as llama.cpp.\n",
    "- Extend `TOOL_REGISTRY` with more capabilities and update the Outlines discriminated union to benchmark richer agents.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

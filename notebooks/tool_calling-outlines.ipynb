{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcd94af",
   "metadata": {},
   "source": [
    "# Tool Calling with Guidance and Outlines\n",
    "\n",
    "This notebook mirrors the original `docs/tutorials/guidance/tool_calling.ipynb` tutorial and extends it with an Outlines-based implementation so we can evaluate both stacks side-by-side. The sections follow the agreed outline and are intended to be executed top-to-bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc748c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "We install or verify the required dependencies and configure shared imports. Guidance and Outlines both rely on a working LLM backend (OpenAI, vLLM, llama.cpp, etc.). The code assumes an OpenAI-compatible HTTP API is available via environment variables such as `OPENAI_API_KEY`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a883be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies into the current environment.\n",
    "# Uncomment the lines below if you need to install packages.\n",
    "# %pip install guidance outlines google-genai aisuite llama-cpp-python httpx sympy pydantic pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79a4a9",
   "metadata": {},
   "source": [
    "## 2. Replicate Guidance Tool-Calling Workflow\n",
    "\n",
    "We reuse the original tutorial's tools to confirm the baseline behaviour before porting anything to Outlines. The cells below define the helpers, register them with Guidance, and (optionally) execute a request against a live OpenAI-compatible endpoint. Set `RUN_GUIDANCE_DEMO = True` if you have working OpenAI credentials; otherwise the demo remains skipped by default while we rely on Gemini for Outlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "from ast import literal_eval\n",
    "from functools import lru_cache\n",
    "from typing import Literal\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from pydantic import BaseModel, ConfigDict, Field\n",
    "\n",
    "try:\n",
    "    from openai import BadRequestError\n",
    "except ImportError:  # pragma: no cover - optional dependency\n",
    "    BadRequestError = Exception  # type: ignore[assignment]\n",
    "\n",
    "try:\n",
    "    from sympy import sympify\n",
    "except ImportError:  # pragma: no cover - optional dependency\n",
    "    sympify = None  # type: ignore[assignment]\n",
    "\n",
    "RNG = random.SystemRandom()\n",
    "DOMAIN_PATTERN = re.compile(r\"(?:(?:[A-Za-z0-9-]+\\.)+[A-Za-z]{2,})\")\n",
    "WHOIS_EXECUTABLE = shutil.which(\"whois\")\n",
    "\n",
    "\n",
    "def get_weather_func(\n",
    "    city: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"celsius\"\n",
    ") -> str:\n",
    "    \"\"\"Return a mock weather string so the demos stay deterministic-friendly.\"\"\"\n",
    "    temp = RNG.randint(-10, 35)\n",
    "    weather = \"snowy\" if temp <= 0 else \"cloudy\" if temp <= 20 else \"sunny\"\n",
    "    if unit == \"fahrenheit\":\n",
    "        temp = temp * 9 / 5 + 32\n",
    "    return (\n",
    "        f\"The current temperature in {city} is {temp} degrees \"\n",
    "        f\"{unit} and it is {weather}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def whois_lookup(url: str) -> str:\n",
    "    \"\"\"Perform a WHOIS lookup for the provided URL and return the raw output.\"\"\"\n",
    "    if WHOIS_EXECUTABLE is None:\n",
    "        msg = \"The 'whois' executable is not available on this system.\"\n",
    "        raise RuntimeError(msg)\n",
    "    domain = urlparse(url).netloc or url\n",
    "    if not domain or not DOMAIN_PATTERN.fullmatch(domain):\n",
    "        msg = f\"Invalid domain: {domain}\"\n",
    "        raise ValueError(msg)\n",
    "    result = subprocess.run(  # noqa: S603\n",
    "        [WHOIS_EXECUTABLE, domain],\n",
    "        check=False,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=30,\n",
    "    )\n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "def evaluate_expression(expr: str) -> str:\n",
    "    \"\"\"Safely evaluate a simple arithmetic expression using sympy as a guard.\"\"\"\n",
    "    if sympify is None:\n",
    "        msg = \"Install sympy to enable the calculator tool.\"\n",
    "        raise RuntimeError(msg)\n",
    "    sympy_expr = sympify(expr)\n",
    "    if not sympy_expr.is_number:\n",
    "        msg = f\"Invalid expression: {expr}\"\n",
    "        raise ValueError(msg)\n",
    "    return str(literal_eval(str(sympy_expr)))\n",
    "\n",
    "\n",
    "class GetWeatherArgs(BaseModel):\n",
    "    \"\"\"Structured input expected by the weather tool.\"\"\"\n",
    "\n",
    "    city: str = Field(description=\"The city to inspect.\")\n",
    "    unit: Literal[\"celsius\", \"fahrenheit\"] = Field(description=\"Unit selection.\")\n",
    "\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "\n",
    "RUN_GUIDANCE_DEMO = os.getenv(\"RUN_GUIDANCE_DEMO\", \"false\").lower() == \"true\"\n",
    "GUIDANCE_MODEL_NAME = os.getenv(\"GUIDANCE_MODEL_NAME\", \"gpt-5-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f17e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import Tool, assistant, gen, user\n",
    "from guidance.models import OpenAI\n",
    "\n",
    "get_weather_tool = Tool.from_callable(\n",
    "    callable=get_weather_func,\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a given city.\",\n",
    "    parameters=GetWeatherArgs,\n",
    ")\n",
    "whois_tool = Tool.from_regex(\n",
    "    pattern=r\"https?:\\/\\/[^\\s]+\",\n",
    "    callable=whois_lookup,\n",
    "    name=\"whois_lookup\",\n",
    "    description=\"Perform a WHOIS lookup on a URL.\",\n",
    ")\n",
    "calculator_tool = Tool.from_callable(\n",
    "    callable=evaluate_expression,\n",
    "    name=\"calculator\",\n",
    "    description=\"Evaluate a basic arithmetic expression.\",\n",
    ")\n",
    "guidance_tools = [get_weather_tool, whois_tool, calculator_tool]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_guidance_model() -> OpenAI:\n",
    "    \"\"\"Instantiate and cache the Guidance model backend.\"\"\"\n",
    "    return OpenAI(GUIDANCE_MODEL_NAME, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92095795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenAI rejected the streaming request (often due to unverified org). Set RUN_GUIDANCE_DEMO = False or switch to a streaming-capable backend.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_guidance_demo(prompt: str) -> str:\n",
    "    \"\"\"Execute a guidance tool call while handling optional API failures gracefully.\"\"\"\n",
    "    if not RUN_GUIDANCE_DEMO:\n",
    "        return \"Skipping live Guidance call. Set RUN_GUIDANCE_DEMO = True to execute.\"\n",
    "    lm = get_guidance_model()\n",
    "    try:\n",
    "        with user():\n",
    "            lm += prompt\n",
    "        with assistant():\n",
    "            lm += gen(tools=guidance_tools, tool_choice=\"required\")\n",
    "        with assistant():\n",
    "            lm += gen()\n",
    "        return str(lm)\n",
    "    except BadRequestError:\n",
    "        return (\n",
    "            \"OpenAI rejected the streaming request (often due to unverified org). \"\n",
    "            \"Set RUN_GUIDANCE_DEMO = False or switch to a streaming-capable backend.\"\n",
    "        )\n",
    "\n",
    "\n",
    "guidance_demo_prompt = \"Who owns the openai.com website?\"\n",
    "run_guidance_demo(guidance_demo_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39836337",
   "metadata": {},
   "source": [
    "## 3. Port the Workflow to Outlines\n",
    "\n",
    "Outlines does not execute callbacks automatically, so we compile a structured schema that asks the model which tool to call and with what arguments. The runtime then invokes the Python callable, records the observation, and loops until a final answer is produced. This mirrors the behaviour of Guidance while keeping the decoding constrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a82f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections.abc import Callable\n",
    "from enum import Enum\n",
    "from typing import Annotated, Any\n",
    "\n",
    "from outlines import Generator, json_schema\n",
    "from pydantic import TypeAdapter\n",
    "\n",
    "from gvo.llm import get_outlines_runtime\n",
    "\n",
    "OUTLINES_PROVIDER = (\n",
    "    os.getenv(\"GVO_OUTLINES_PROVIDER\") or os.getenv(\"OUTLINES_PROVIDER\") or \"gemini\"\n",
    ").lower()\n",
    "if OUTLINES_PROVIDER not in {\"gemini\", \"openai\"}:\n",
    "    OUTLINES_PROVIDER = \"gemini\"\n",
    "\n",
    "_DEFAULT_OUTLINES_MODELS = {\n",
    "    \"gemini\": \"gemini-1.5-flash\",\n",
    "    \"openai\": \"gpt-5-mini\",\n",
    "}\n",
    "\n",
    "RUN_OUTLINES_DEMO = True\n",
    "OUTLINES_MODEL_NAME = os.getenv(\n",
    "    \"OUTLINES_MODEL_NAME\", _DEFAULT_OUTLINES_MODELS[OUTLINES_PROVIDER]\n",
    ")\n",
    "\n",
    "\n",
    "class ToolName(str, Enum):\n",
    "    \"\"\"Enumerate the available tool names exposed to the model.\"\"\"\n",
    "\n",
    "    get_weather = \"get_weather\"\n",
    "    whois_lookup = \"whois_lookup\"\n",
    "    calculator = \"calculator\"\n",
    "    final_answer = \"final_answer\"\n",
    "\n",
    "\n",
    "class WeatherCall(BaseModel):\n",
    "    \"\"\"Payload for the weather lookup tool.\"\"\"\n",
    "\n",
    "    tool: Literal[\"get_weather\"] = \"get_weather\"\n",
    "    arguments: GetWeatherArgs\n",
    "\n",
    "\n",
    "class WhoisArgs(BaseModel):\n",
    "    \"\"\"Arguments expected by the WHOIS lookup tool.\"\"\"\n",
    "\n",
    "    url: str = Field(pattern=r\"https?://[^\\s]+\", description=\"Absolute URL to inspect.\")\n",
    "\n",
    "\n",
    "class WhoisCall(BaseModel):\n",
    "    \"\"\"Payload for the WHOIS lookup tool.\"\"\"\n",
    "\n",
    "    tool: Literal[\"whois_lookup\"] = \"whois_lookup\"\n",
    "    arguments: WhoisArgs\n",
    "\n",
    "\n",
    "class CalculatorArgs(BaseModel):\n",
    "    \"\"\"Arguments expected by the calculator tool.\"\"\"\n",
    "\n",
    "    expression: str = Field(description=\"Arithmetic expression such as 2+2\")\n",
    "\n",
    "\n",
    "class CalculatorCall(BaseModel):\n",
    "    \"\"\"Payload for the calculator tool.\"\"\"\n",
    "\n",
    "    tool: Literal[\"calculator\"] = \"calculator\"\n",
    "    arguments: CalculatorArgs\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"Terminal payload emitted when no more tools are required.\"\"\"\n",
    "\n",
    "    tool: Literal[\"final_answer\"] = \"final_answer\"\n",
    "    answer: str = Field(description=\"The final natural language response.\")\n",
    "\n",
    "\n",
    "ToolDecisionType = WeatherCall | WhoisCall | CalculatorCall | FinalAnswer\n",
    "\n",
    "TOOL_DECISION_SCHEMA = json_schema(TypeAdapter(ToolDecisionType).json_schema())\n",
    "\n",
    "TOOL_REGISTRY: dict[str, Callable[[dict[str, Any]], str]] = {\n",
    "    \"get_weather\": lambda payload: get_weather_func(**payload),\n",
    "    \"whois_lookup\": lambda payload: whois_lookup(payload[\"url\"]),\n",
    "    \"calculator\": lambda payload: str(evaluate_expression(payload[\"expression\"])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7821eb25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': '* GenerateContentRequest.generation_config.response_schema.properties: should be non-empty for OBJECT type\\n', 'status': 'INVALID_ARGUMENT'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mReached max turns without final answer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m outlines_demo_prompt = (\n\u001b[32m     79\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIf Johnny has 5 apples and gives 2 to Mary, how many apples remain?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     80\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[43mrun_outlines_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutlines_demo_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mrun_outlines_agent\u001b[39m\u001b[34m(question, max_turns, inference_overrides)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inference_overrides:\n\u001b[32m     54\u001b[39m     inference_kwargs.update(inference_overrides)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m raw_decision = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m decision = _normalize_decision(raw_decision)\n\u001b[32m     57\u001b[39m tool_name = decision.get(\u001b[33m\"\u001b[39m\u001b[33mtool\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/outlines/generator.py:67\u001b[39m, in \u001b[36mBlackBoxGenerator.__call__\u001b[39m\u001b[34m(self, prompt, **inference_kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: Any, **inference_kwargs) -> Any:\n\u001b[32m     52\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate a response from the model.\u001b[39;00m\n\u001b[32m     53\u001b[39m \n\u001b[32m     54\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m \n\u001b[32m     66\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/outlines/models/gemini.py:297\u001b[39m, in \u001b[36mGemini.generate\u001b[39m\u001b[34m(self, model_input, output_type, **inference_kwargs)\u001b[39m\n\u001b[32m    294\u001b[39m contents = \u001b[38;5;28mself\u001b[39m.type_adapter.format_input(model_input)\n\u001b[32m    295\u001b[39m generation_config = \u001b[38;5;28mself\u001b[39m.type_adapter.format_output_type(output_type)\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m completion = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_kwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m completion.text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/google/genai/models.py:5001\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4999\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5000\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5001\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5002\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5003\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5005\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5006\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/google/genai/models.py:3813\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3810\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   3811\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3813\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3814\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3815\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3817\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   3818\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3819\u001b[39m ):\n\u001b[32m   3820\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/google/genai/_api_client.py:1307\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1298\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1299\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1302\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1303\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1304\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1305\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1306\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1308\u001b[39m   response_body = (\n\u001b[32m   1309\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1310\u001b[39m   )\n\u001b[32m   1311\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/google/genai/_api_client.py:1143\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1140\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/google/genai/_api_client.py:1120\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1113\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1114\u001b[39m       method=http_request.method,\n\u001b[32m   1115\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1119\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1120\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1121\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1122\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1123\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/gvo/.venv/lib/python3.12/site-packages/google/genai/errors.py:108\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    106\u001b[39m status_code = response.status_code\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    110\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': '* GenerateContentRequest.generation_config.response_schema.properties: should be non-empty for OBJECT type\\n', 'status': 'INVALID_ARGUMENT'}}"
     ]
    }
   ],
   "source": [
    "OUTLINES_SYSTEM_PROMPT = (\n",
    "    \"You are an assistant that can call tools to answer the user's question.\\n\"\n",
    "    \"Use the provided tool names exactly. When you have enough information, \"\n",
    "    \"return `final_answer`.\"\n",
    ")\n",
    "\n",
    "\n",
    "def render_transcript(question: str, scratchpad: str) -> str:\n",
    "    \"\"\"Format the conversation transcript expected by the Outlines runner.\"\"\"\n",
    "    base = f\"{OUTLINES_SYSTEM_PROMPT}\\n\\nQuestion: {question}\\nScratchpad:\"\n",
    "    if scratchpad:\n",
    "        return f\"{base}\\n{scratchpad.strip()}\"\n",
    "    return base\n",
    "\n",
    "\n",
    "def _coerce_payload(payload: object) -> object:\n",
    "    \"\"\"Convert pydantic models or other objects into plain dictionaries.\"\"\"\n",
    "    if hasattr(payload, \"model_dump\"):\n",
    "        return payload.model_dump()\n",
    "    return payload\n",
    "\n",
    "\n",
    "def _normalize_decision(decision: object) -> dict[str, object]:\n",
    "    \"\"\"Normalise the model response into a dictionary for downstream handling.\"\"\"\n",
    "    if isinstance(decision, str):\n",
    "        return json.loads(decision)\n",
    "    if isinstance(decision, dict):\n",
    "        return {key: _coerce_payload(value) for key, value in decision.items()}\n",
    "    coerced = _coerce_payload(decision)\n",
    "    if isinstance(coerced, dict):\n",
    "        return coerced\n",
    "    msg = f\"Unexpected decision payload type: {type(decision)!r}\"\n",
    "    raise TypeError(msg)\n",
    "\n",
    "\n",
    "def run_outlines_agent(\n",
    "    question: str,\n",
    "    max_turns: int = 5,\n",
    "    inference_overrides: dict[str, object] | None = None,\n",
    ") -> str:\n",
    "    \"\"\"Loop until the model emits `final_answer` or `max_turns` turns elapse.\"\"\"\n",
    "    if not RUN_OUTLINES_DEMO:\n",
    "        return \"Skipping live Outlines call. Set RUN_OUTLINES_DEMO = True to execute.\"\n",
    "    try:\n",
    "        runtime = get_outlines_runtime()\n",
    "    except RuntimeError as exc:\n",
    "        return f\"Failed to initialize Outlines model: {exc}\"\n",
    "    generator = Generator(runtime.model, TOOL_DECISION_SCHEMA)\n",
    "    scratchpad = \"\"\n",
    "    for turn in range(max_turns):\n",
    "        prompt = render_transcript(question, scratchpad)\n",
    "        inference_kwargs = dict(runtime.inference_defaults)\n",
    "        if inference_overrides:\n",
    "            inference_kwargs.update(inference_overrides)\n",
    "        raw_decision = generator(prompt, **inference_kwargs)\n",
    "        decision = _normalize_decision(raw_decision)\n",
    "        tool_name = decision.get(\"tool\")\n",
    "        if tool_name == ToolName.final_answer.value:\n",
    "            return str(decision.get(\"answer\", \"\"))\n",
    "        arguments = _coerce_payload(decision.get(\"arguments\", {})) or {}\n",
    "        executor = TOOL_REGISTRY.get(tool_name)\n",
    "        if executor is None:\n",
    "            observation = f\"Unknown tool: {tool_name}\"\n",
    "        else:\n",
    "            try:\n",
    "                observation = executor(arguments)\n",
    "            except (RuntimeError, ValueError) as exc:\n",
    "                observation = f\"Tool error: {exc}\"\n",
    "        scratchpad += (\n",
    "            f\"\\nTurn {turn + 1}\"\n",
    "            f\"\\nTool: {tool_name}\"\n",
    "            f\"\\nArgs: {json.dumps(arguments)}\"\n",
    "            f\"\\nObservation: {observation}\"\n",
    "        )\n",
    "    return \"Reached max turns without final answer.\"\n",
    "\n",
    "\n",
    "outlines_demo_prompt = (\n",
    "    \"If Johnny has 5 apples and gives 2 to Mary, how many apples remain?\"\n",
    ")\n",
    "run_outlines_agent(outlines_demo_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc2001",
   "metadata": {},
   "source": [
    "## 4. Side-by-Side Harness\n",
    "\n",
    "This helper runs the same question through both stacks (respecting the demo flags) so you can diff the traces and assess parity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3a501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guidance': 'OpenAI rejected the streaming request (often due to unverified org). Set RUN_GUIDANCE_DEMO = False or switch to a streaming-capable backend.',\n",
       " 'outlines': 'Skipping live Outlines call. Set RUN_OUTLINES_DEMO = True to execute.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_workflows(question: str) -> dict[str, str]:\n",
    "    \"\"\"Execute the same query via Guidance and Outlines runners.\"\"\"\n",
    "    return {\n",
    "        \"guidance\": run_guidance_demo(question),\n",
    "        \"outlines\": run_outlines_agent(question),\n",
    "    }\n",
    "\n",
    "\n",
    "comparison_prompt = \"What is the weather like in San Francisco today?\"\n",
    "compare_workflows(comparison_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c15cd",
   "metadata": {},
   "source": [
    "## 5. Where to Go Next\n",
    "\n",
    "- Toggle `RUN_GUIDANCE_DEMO` and `RUN_OUTLINES_DEMO` to compare real traces once credentials are set up.\n",
    "- Swap `GUIDANCE_MODEL_NAME` or set `GVO_OUTLINES_PROVIDER`/`OUTLINES_MODEL_NAME` to experiment with other backends (e.g. OpenAI or self-hosted llama.cpp).\n",
    "- Extend `TOOL_REGISTRY` with more capabilities and update the Outlines discriminated union to benchmark richer agents.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redteamer-zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
